{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocal-coverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for anger.\n",
      "Loaded 1326 raw labelled audio data samples.\n",
      "Loading data for happy.\n",
      "Loaded 1181 raw labelled audio data samples.\n",
      "Loading data for sad.\n",
      "Loaded 1176 raw labelled audio data samples.\n",
      "Found data for 3 speakers : anger, happy, sad\n",
      "Extracting features and labels for 3683 audio windows...\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from features import FeatureExtractor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "# %%---------------------------------------------------------------------------\n",
    "#\n",
    "#\t\t                 Load Data From Disk\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "data_dir = 'data' # directory where the data files are stored\n",
    "\n",
    "output_dir = 'training_output' # directory where the classifier(s) are stored\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "\tos.mkdir(output_dir)\n",
    "\n",
    "# the filenames should be in the form 'speaker-data-subject-1.csv', e.g. 'speaker-data-Erik-1.csv'.\n",
    "\n",
    "class_names = [] # the set of classes, i.e. speakers\n",
    "\n",
    "data = np.zeros((0,8002)) #8002 = 1 (timestamp) + 8000 (for 8kHz audio data) + 1 (label)\n",
    "for filename in os.listdir(data_dir):\n",
    "\tif filename.endswith(\".csv\") and filename.startswith(\"speaker-data\"):\n",
    "\t\tfilename_components = filename.split(\"-\") # split by the '-' character\n",
    "\t\tspeaker = filename_components[2]\n",
    "\t\tprint(\"Loading data for {}.\".format(speaker))\n",
    "\t\tif speaker not in class_names:\n",
    "\t\t\tclass_names.append(speaker)\n",
    "\t\tspeaker_label = class_names.index(speaker)\n",
    "\t\tsys.stdout.flush()\n",
    "\t\tdata_file = os.path.join(data_dir, filename)\n",
    "\t\tdata_for_current_speaker = np.genfromtxt(data_file, delimiter=',')\n",
    "\t\tprint(\"Loaded {} raw labelled audio data samples.\".format(len(data_for_current_speaker)))\n",
    "\t\tsys.stdout.flush()\n",
    "\t\tdata = np.append(data, data_for_current_speaker, axis=0)\n",
    "\n",
    "print(\"Found data for {} speakers : {}\".format(len(class_names), \", \".join(class_names)))\n",
    "# %%---------------------------------------------------------------------------\n",
    "#\n",
    "#\t\t                Extract Features & Labels\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Update this depending on how you compute your features\n",
    "n_features = 988\n",
    "\n",
    "print(\"Extracting features and labels for {} audio windows...\".format(data.shape[0]))\n",
    "sys.stdout.flush()\n",
    "\n",
    "X = np.zeros((0,n_features))\n",
    "y = np.zeros(0,)\n",
    "\n",
    "# change debug to True to show print statements we've included:\n",
    "feature_extractor = FeatureExtractor(debug=False) \n",
    "\n",
    "nr_total_windows = 0\n",
    "nr_bad_windows = 0\n",
    "nr_windows_with_zeros = 0\n",
    "\n",
    "for i,window_with_timestamp_and_label in enumerate(data):\n",
    "    window = window_with_timestamp_and_label[1:-1]\n",
    "    label = data[i,-1]\n",
    "    nr_total_windows += 1\n",
    "    try:\n",
    "        x = feature_extractor.extract_features(window)\n",
    "        if (len(x) != X.shape[1]):\n",
    "            print(\"Received feature vector of length {}. Expected feature vector of length {}.\".format(len(x), X.shape[1]))\n",
    "        X = np.append(X, np.reshape(x, (1,-1)), axis=0)\n",
    "        y = np.append(y, label)\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "        nr_bad_windows += 1\n",
    "        if np.all((window == 0)):\n",
    "            nr_windows_with_zeros += 1\n",
    "print(\"{} windows found\".format(nr_total_windows))\n",
    "print(\"{} bad windows found, with {} windows with only zeros\".format(nr_bad_windows, nr_windows_with_zeros))\n",
    "    \n",
    "print(\"Finished feature extraction over {} windows\".format(len(X)))\n",
    "print(\"Unique labels found: {}\".format(set(y)))\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-privilege",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-ireland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%---------------------------------------------------------------------------\n",
    "#\n",
    "#\t\t                Train & Evaluate Classifier\n",
    "#\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "n = len(y)\n",
    "n_classes = len(class_names)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"---------------------- Decision Tree -------------------------\")\n",
    "\n",
    "total_accuracy = 0.0\n",
    "total_precision = [0.0, 0.0, 0.0]\n",
    "total_recall = [0.0, 0.0, 0.0]\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=None)\n",
    "for i, (train_index, test_index) in enumerate(cv.split(X)):\n",
    "\tX_train, X_test = X[train_index], X[test_index]\n",
    "\ty_train, y_test = y[train_index], y[test_index]\n",
    "\ttree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4)\n",
    "\tprint(\"Fold {} : Training decision tree classifier over {} points...\".format(i, len(y_train)))\n",
    "\tsys.stdout.flush()\n",
    "\ttree.fit(X_train, y_train)\n",
    "\tprint(\"Evaluating classifier over {} points...\".format(len(y_test)))\n",
    "\n",
    "\t# predict the labels on the test data\n",
    "\ty_pred = tree.predict(X_test)\n",
    "\n",
    "\t# show the comparison between the predicted and ground-truth labels\n",
    "\tconf = confusion_matrix(y_test, y_pred, labels=[0,1,2])\n",
    "\n",
    "\taccuracy = np.sum(np.diag(conf)) / float(np.sum(conf))\n",
    "\tprecision = np.nan_to_num(np.diag(conf) / np.sum(conf, axis=1).astype(float))\n",
    "\trecall = np.nan_to_num(np.diag(conf) / np.sum(conf, axis=0).astype(float))\n",
    "\n",
    "\ttotal_accuracy += accuracy\n",
    "\ttotal_precision += precision\n",
    "\ttotal_recall += recall\n",
    "   \n",
    "print(\"The average accuracy is {}\".format(total_accuracy/10.0))  \n",
    "print(\"The average precision is {}\".format(total_precision/10.0))    \n",
    "print(\"The average recall is {}\".format(total_recall/10.0))  \n",
    "\n",
    "print(\"Training decision tree classifier on entire dataset...\")\n",
    "tree.fit(X, y)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"---------------------- Random Forest Classifier -------------------------\")\n",
    "total_accuracy = 0.0\n",
    "total_precision = [0.0, 0.0, 0.0]\n",
    "total_recall = [0.0, 0.0, 0.0]\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(cv.split(X)):\n",
    "\tX_train, X_test = X[train_index], X[test_index]\n",
    "\ty_train, y_test = y[train_index], y[test_index]\n",
    "\tprint(\"Fold {} : Training Random Forest classifier over {} points...\".format(i, len(y_train)))\n",
    "\tsys.stdout.flush()\n",
    "\tclf = RandomForestClassifier(n_estimators=200, max_features= \"log2\")\n",
    "\tclf.fit(X_train, y_train)\n",
    "\n",
    "\tprint(\"Evaluating classifier over {} points...\".format(len(y_test)))\n",
    "\t# predict the labels on the test data\n",
    "\ty_pred = clf.predict(X_test)\n",
    "\n",
    "\t# show the comparison between the predicted and ground-truth labels\n",
    "\tconf = confusion_matrix(y_test, y_pred, labels=[0,1,2])\n",
    "\n",
    "\taccuracy = np.sum(np.diag(conf)) / float(np.sum(conf))\n",
    "\tprecision = np.nan_to_num(np.diag(conf) / np.sum(conf, axis=1).astype(float))\n",
    "\trecall = np.nan_to_num(np.diag(conf) / np.sum(conf, axis=0).astype(float))\n",
    "\n",
    "\ttotal_accuracy += accuracy\n",
    "\ttotal_precision += precision\n",
    "\ttotal_recall += recall\n",
    "   \n",
    "print(\"The average accuracy is {}\".format(total_accuracy/10.0))  \n",
    "print(\"The average precision is {}\".format(total_precision/10.0))    \n",
    "print(\"The average recall is {}\".format(total_recall/10.0))  \n",
    "\n",
    "# TODO: (optional) train other classifiers and print the average metrics using 10-fold cross-validation\n",
    "\n",
    "# Set this to the best model you found, trained on all the data:\n",
    "best_classifier = RandomForestClassifier(n_estimators=100, max_features= \"log2\")\n",
    "best_classifier.fit(X,y) \n",
    "\n",
    "classifier_filename='classifier.pickle'\n",
    "print(\"Saving best classifier to {}...\".format(os.path.join(output_dir, classifier_filename)))\n",
    "with open(os.path.join(output_dir, classifier_filename), 'wb') as f: # 'wb' stands for 'write bytes'\n",
    "\tpickle.dump(best_classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_feature_importances_mydata(model):\n",
    "#  n_features = X_train.shape[1]\n",
    "#  plt.barh(range(n_features), model.feature_importances_, align='center')\n",
    "#  plt.yticks(np.arange(n_features), list(mydata1)[:-1])\n",
    "#  plt.xlabel(\"Variable importance\")\n",
    "#  plt.ylabel(\"Independent Variable\")\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# matplotlib.rcParams.update({'font.size': 15})\n",
    "# plt.figure(figsize=(15,10))\n",
    "# plot_feature_importances_mydata(tree)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib.rcParams.update({'font.size': 15})\n",
    "# plt.figure(figsize=(15,10))\n",
    "# plot_feature_importances_mydata(clf)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-extension",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
